// Importamos los clientes de Google Cloud necesarios
import { Firestore } from "@google-cloud/firestore";
import { VertexAI } from "@google-cloud/vertexai";
import { v1 } from "@google-cloud/discoveryengine";
const discoveryengineClient = new v1.SearchServiceClient();

// Leemos las variables de entorno necesarias
const projectId = process.env.NEXT_PUBLIC_FIREBASE_PROJECT_ID;
const location = process.env.VERTEX_AI_LOCATION || "us-central1"; // RegiÃ³n por defecto
const firestoreDatabaseId = process.env.FIRESTORE_DATABASE_ID || "(default)";
const vertexAiSearchDataStoreId = process.env.VERTEX_AI_SEARCH_DATA_STORE_ID;

import { auth } from 'google-auth-library';

const authClient = await auth.getClient();
const project = await auth.getProjectId();
console.log("âœ… Autenticado en proyecto:", project);



if (!process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON) {
  throw new Error("âŒ GOOGLE_APPLICATION_CREDENTIALS_JSON no estÃ¡ definida.");
}

const parsedCredentials = JSON.parse(process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON);

// Verificamos que las variables clave estÃ©n definidas
if (
  !projectId ||
  !process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON ||
  !vertexAiSearchDataStoreId
) {
  console.error("âŒ Faltan variables de entorno requeridas.");
  // En producciÃ³n deberÃ­as retornar un error HTTP aquÃ­.
}

// ðŸ”¥ Inicializamos el cliente de Firestore
const firestore = new Firestore({
  projectId: projectId,
  databaseId: firestoreDatabaseId,
  credentials: parsedCredentials,
});

// ðŸ¤– Inicializamos el cliente de Vertex AI
const vertex_ai = new VertexAI({
  project: projectId,
  location: location,
  credentials: parsedCredentials,
});

// ðŸ“¦ Construimos el nombre completo del data store para las bÃºsquedas
const dataStoreName = `projects/${projectId}/locations/global/collections/default_collection/dataStores/${vertexAiSearchDataStoreId}`;

// âœ¨ Creamos el modelo generativo Gemini-Pro
const generativeModel = vertex_ai.getGenerativeModel({
    model: 'gemini-2.0-flash-lite-001',
});

// Handler para el endpoint /api/chat (Next.js o API Route en App Router)
export async function POST(req) {
  const { query: userQuery } = await req.json();

  // Saludo inicial o mensaje vacÃ­o
  if (userQuery.trim() === "" || userQuery.toLowerCase().includes("hola")) {
    const bienvenida = `Â¡Hola! ðŸ‘‹ Un placer saludarte. 
Â¿En quÃ© puedo ayudarte hoy a encontrar una propiedad? Â¿Tienes alguna idea de lo que estÃ¡s buscando? Â¿Por ejemplo, te interesa comprar, alquilar, en quÃ© zona te gustarÃ­a, quÃ© tipo de propiedad tienes en mente (piso, casa, local comercial...)? CuÃ©ntame un poco mÃ¡s sobre tus necesidades para poder ayudarte mejor. ðŸ˜Š`;

    return new Response(
      JSON.stringify({
        conversationalResponse: bienvenida,
        propertyResults: [],
      }),
      {
        status: 200,
        headers: { "Content-Type": "application/json" },
      }
    );
  }

  if (!userQuery) {
    return new Response(
      JSON.stringify({ error: "Missing user query in request body" }),
      {
        status: 400,
        headers: { "Content-Type": "application/json" },
      }
    );
  }

  let conversationalResponse = "Lo siento, no pude procesar tu solicitud.";
  let propertyResults = [];
  let searchParameters = {};

  try {
    // ðŸ§  Paso 1: Gemini intenta extraer criterios de bÃºsqueda
    const prompt = `Eres un asistente de IA que extrae criterios de bÃºsqueda de propiedades a partir del mensaje del usuario. Tu respuesta debe ser un objeto JSON con posibles campos como "municipio_catastro", "minPrice", "maxPrice", "rooms", "Property Type". Si no se encuentran criterios o el usuario hace una pregunta que no se puede traducir directamente a criterios de bÃºsqueda, devuelve un objeto JSON vacÃ­o {}. Solo incluye campos para los que el usuario haya proporcionado explÃ­citamente un valor o rango.

Mensaje del usuario: "${userQuery}"

Salida JSON:
`;

    const geminiResponse = await generativeModel.generateContent(prompt);
console.log('ðŸ” Respuesta cruda desde Vertex AI:', JSON.stringify(geminiResponse, null, 2));
    const geminiText =
      geminiResponse.response.candidates[0]?.content?.parts[0]?.text;

    try {
      // Extraemos el JSON desde el texto generado por Gemini (puede tener ruido o formato suelto)
      const jsonMatch = geminiText.match(/\{.*\}/s);
      if (jsonMatch && jsonMatch[0]) {
        searchParameters = JSON.parse(jsonMatch[0]);
      } else {
        console.warn("âš ï¸ Gemini no devolviÃ³ JSON vÃ¡lido:", geminiText);
        const fallbackPrompt = `El usuario dijo: "${userQuery}". No pude extraer criterios de bÃºsqueda especÃ­ficos. Responde de manera conversacional, en espaÃ±ol, y pÃ­dele mÃ¡s informaciÃ³n. Usa formato Markdown para facilitar la lectura.`;
        const fallbackResponse = await generativeModel.generateContent(
          fallbackPrompt
        );
        conversationalResponse =
          fallbackResponse.response.candidates[0]?.content?.parts[0]?.text;
        return new Response(
          JSON.stringify({ conversationalResponse, propertyResults }),
          {
            status: 200,
            headers: { "Content-Type": "application/json" },
          }
        );
      }
    } catch (parseError) {
      console.error("âŒ Error al parsear el JSON de Gemini:", parseError);
      console.log("Texto bruto de Gemini:", geminiText);
      const fallbackPrompt = `El usuario dijo: "${userQuery}". Hubo un error al procesar la solicitud. Genera una respuesta en espaÃ±ol, amigable, pidiendo disculpas por el error. Usa Markdown para dar formato.`;
      const fallbackResponse = await generativeModel.generateContent(
        fallbackPrompt
      );
      conversationalResponse =
        fallbackResponse.response.candidates[0]?.content?.parts[0]?.text;
      return new Response(
        JSON.stringify({
          error: "Error processing query.",
          conversationalResponse,
        }),
        {
          status: 500,
          headers: { "Content-Type": "application/json" },
        }
      );
    }

    // ðŸ” Paso 2: Si hay parÃ¡metros vÃ¡lidos, hacemos bÃºsqueda en Vertex AI Search
    if (Object.keys(searchParameters).length > 0) {
      console.log("ðŸ”Ž ParÃ¡metros de bÃºsqueda extraÃ­dos:", searchParameters);

      const searchRequest = {
        servingConfig: `${dataStoreName}/servingConfigs/default_serving_config`,
        query: userQuery,
        queryExpansionSpec: { condition: "AUTO" },
        spellCorrectionSpec: { mode: "AUTO" },
        filter: buildVertexAISearchFilter(searchParameters),
      };

      console.log(
        "ðŸ›  Enviando bÃºsqueda a Vertex AI Search:",
        JSON.stringify(searchRequest, null, 2)
      );

      const [searchResponse] = await discoveryengineClient.search(
        searchRequest
      );

      // Extraemos los resultados del documento (segÃºn cÃ³mo configuraste la indexaciÃ³n)
      propertyResults =
        searchResponse.results
          ?.map((result) => {
            try {
              // Verifica que `result.document.content?.jsonData` exista
              const rawJson = result.document?.content?.jsonData;
              return rawJson ? JSON.parse(rawJson) : null;
            } catch (e) {
              console.error("âŒ Error parseando resultado:", e);
              return null;
            }
          })
          .filter((item) => item !== null) || [];

      console.log(`âœ… ${propertyResults.length} propiedades encontradas.`);

      // ðŸ§  Paso 3: Creamos una respuesta conversacional usando Gemini
      if (propertyResults.length > 0) {
        const resultSummaryPrompt = `El usuario busca propiedades basÃ¡ndose en: "${userQuery}". He encontrado ${
          propertyResults.length
        } propiedades. AquÃ­ tienes un resumen de las primeras 3 para generar una respuesta conversacional en espaÃ±ol (usa Markdown para formato):

  - Menciona cuÃ¡ntas propiedades se encontraron.
  - Resalta 2-3 caracterÃ­sticas clave de cada una.
  - Termina animando al usuario a consultar mÃ¡s o a afinar su bÃºsqueda.
  `;

        const conversationalResponseGen = await generativeModel.generateContent(
          resultSummaryPrompt
        );
        conversationalResponse =
          conversationalResponseGen.response.candidates[0]?.content?.parts[0]
            ?.text;
      } else {
        const noResultsPrompt = `El usuario buscÃ³ propiedades con el mensaje: "${userQuery}", pero no se encontraron resultados. Genera un mensaje conversacional y amable en espaÃ±ol, explicando la situaciÃ³n y sugiriendo:
  
  - Probar con un presupuesto distinto.
  - Buscar en otra zona cercana.
  - Considerar un tipo de propiedad diferente.

  Usa formato Markdown para claridad.`;
        const noResultsResponse = await generativeModel.generateContent(
          noResultsPrompt
        );
        conversationalResponse =
          noResultsResponse.response.candidates[0]?.content?.parts[0]?.text;
      }
    } else {
      // No se identificaron parÃ¡metros de bÃºsqueda: respuesta genÃ©rica
      const generalResponsePrompt = `El usuario dijo: "${userQuery}". No se identificaron criterios especÃ­ficos de bÃºsqueda. Responde en espaÃ±ol, de forma conversacional, y pregÃºntale cÃ³mo puedes ayudarle a encontrar una propiedad. Usa formato Markdown.`;
      const generalResponse = await generativeModel.generateContent(
        generalResponsePrompt
      );
      conversationalResponse =
        generalResponse.response.candidates[0]?.content?.parts[0]?.text;
    }

    // âœ… Paso 4: Respondemos al frontend
    return new Response(
      JSON.stringify({
        conversationalResponse,
        propertyResults,
      }),
      {
        status: 200,
        headers: { "Content-Type": "application/json" },
      }
    );
  } catch (error) {
    console.error("âŒ Error general en el endpoint /api/chat:", error);

    try {
      const errorPrompt = `Hubo un error tÃ©cnico al procesar la solicitud del usuario: "${userQuery}". Genera una respuesta conversacional en espaÃ±ol, disculpÃ¡ndote por las molestias e invitando al usuario a intentarlo de nuevo mÃ¡s tarde. Usa formato Markdown.`;
      const errorResponseGen = await generativeModel.generateContent(
        errorPrompt
      );
      conversationalResponse =
        errorResponseGen.response.candidates[0]?.content?.parts[0]?.text;
    } catch (genError) {
      console.error("âš ï¸ Fallo generando respuesta de error:", genError);
      conversationalResponse =
        "Lo siento, ha ocurrido un error inesperado. Por favor, intenta de nuevo mÃ¡s tarde.";
    }

    return new Response(
      JSON.stringify({
        error: "Internal Server Error",
        conversationalResponse,
        propertyResults: [],
      }),
      {
        status: 500,
        headers: { "Content-Type": "application/json" },
      }
    );
  }
}
// ðŸ”§ FunciÃ³n auxiliar para construir el filtro de Vertex AI Search
function buildVertexAISearchFilter(params) {
    const filters = [];
  
    if (params.city) {
      filters.push(`municipio_catastro:"${params.city}"`);
    }
    if (params.minPrice) {
      filters.push(`price_approx >= ${params.minPrice}`);
    }
    if (params.maxPrice) {
      filters.push(`price_approx <= ${params.maxPrice}`);
    }
    if (params.bedrooms) {
      filters.push(`rooms >= ${params.bedrooms}`);
    }
    if (params.propertyType) {
      filters.push(`"Property Type":"${params.propertyType}"`);
    }
  
    return filters.join(" AND ");
  }